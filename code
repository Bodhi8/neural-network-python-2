import numpy as np

#sigmoid
def nonlin(x, deriv=False)
if(deriv==true):
  return (x=(1-x))
  
return 1/(1+np.exp(-x))

#input data
x = np.array([[0,0,1],
[0,1,1],
[1,0,1],
[1,1,1]])

y = np.array([[0], 
[1],
[1],
[0]])

#seed

np.random.seed(1)

#synapses(synapses=weights, -1 is the bias for tuning)

syn0 = 2*np.random.random((3,4)) - 1
syn1 = 2*np.random.random((4,1)) - 1

#training(for loop, np.dot is matrix multiplication multiplying synapse matrix by layer) 

for j in xrange(60000:

    #create layers
    l0 = x
    l1 = nonlin(np.dot(l0, syn0))
    l2 = nonlin(np.dot(l1, syn1))
    
    #backpropagation(reduce error each time, error will come out of last layer, 12 is just arbitrary number, 
    if statment 10000 prints every 10000 steps)
    l2_error = y - l2 
    if (j % 10000) == 0:
        print 'Error:' + str(np.mean(np.abs(12_error)))
        
    #calculate delta(the diff in quantity as data moves through layers)
    l2_delta = l2_error*nonlin(l2, deriv=True)
    L1_error = l2_delta.dot(syn1.T)
    l1_delta = l1_error * nonlin(l1, deriv=True)
    
    #update the weights(T flips the matrix) 
    syn1 += l1.T.dot(l2_delta)
    syn2 += l0.T.dot(l1_delta)
    
    print 'output after training'
    print l2
 
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    





